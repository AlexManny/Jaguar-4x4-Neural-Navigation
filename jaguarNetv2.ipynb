{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jaguarNetv2",
      "provenance": [],
      "authorship_tag": "ABX9TyOfA0WeJvDDX/B9KgHKxalL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexManny/licenta_2019/blob/master/jaguarNetv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYMGqD9RC6-A",
        "colab_type": "code",
        "outputId": "f1d11232-1076-4b48-aa1d-79fcaf0c0b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSbW0Y0lDH8m",
        "colab_type": "code",
        "outputId": "bd9d7248-a74d-4a06-c236-dca76742ff76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te7rLdbqDgkA",
        "colab_type": "code",
        "outputId": "dc30020b-e0f7-4756-ba4d-cdb66441502b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kELifnuqDUPy",
        "colab_type": "code",
        "outputId": "bd50b86c-d95a-47ae-8294-af459ba6eee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!git clone https://github.com/qqwweee/keras-yolo3.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-yolo3'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Total 144 (delta 0), reused 0 (delta 0), pack-reused 144\u001b[K\n",
            "Receiving objects: 100% (144/144), 151.07 KiB | 601.00 KiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSP-0wkDZ-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Ma-Dan/keras-yolo4.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx-ZIxtxFkzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/Licenta/jaguarVOC /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuX5o6WQ6ix6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/Licenta/weights /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJN6uEFEHL60",
        "colab_type": "code",
        "outputId": "1b943522-cf6a-43b0-c8de-c1daca27f691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd keras-yolo3/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EasXydmqFkKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python voc_annotationy4.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iJLzrZM7E3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python convert.py yolov3-tiny.cfg yolov3-tiny.weights model_data/tiny_yolo_weights.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8aw51zS2t94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train_jag2.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbqlEV3ZIIDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python yolo_video.py --image --output detection.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvtfqGke4Sq",
        "colab_type": "text"
      },
      "source": [
        "#README qqwweee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rgj5wxUe79C",
        "colab_type": "text"
      },
      "source": [
        "keras-yolo3\n",
        "\n",
        "license\n",
        "Introduction\n",
        "\n",
        "A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.\n",
        "Quick Start\n",
        "\n",
        "    Download YOLOv3 weights from YOLO website.\n",
        "    Convert the Darknet YOLO model to a Keras model.\n",
        "    Run YOLO detection.\n",
        "\n",
        "wget https://pjreddie.com/media/files/yolov3.weights\n",
        "python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5\n",
        "python yolo_video.py [OPTIONS...] --image, for image detection mode, OR\n",
        "python yolo_video.py [video_path] [output_path (optional)]\n",
        "\n",
        "For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file.\n",
        "Usage\n",
        "\n",
        "Use --help to see usage of yolo_video.py:\n",
        "\n",
        "usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]\n",
        "                     [--classes CLASSES] [--gpu_num GPU_NUM] [--image]\n",
        "                     [--input] [--output]\n",
        "\n",
        "positional arguments:\n",
        "  --input        Video input path\n",
        "  --output       Video output path\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help         show this help message and exit\n",
        "  --model MODEL      path to model weight file, default model_data/yolo.h5\n",
        "  --anchors ANCHORS  path to anchor definitions, default\n",
        "                     model_data/yolo_anchors.txt\n",
        "  --classes CLASSES  path to class definitions, default\n",
        "                     model_data/coco_classes.txt\n",
        "  --gpu_num GPU_NUM  Number of GPU to use, default 1\n",
        "  --image            Image detection mode, will ignore all positional arguments\n",
        "\n",
        "    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().\n",
        "\n",
        "Training\n",
        "\n",
        "    Generate your own annotation file and class names file.\n",
        "    One row for one image;\n",
        "    Row format: image_file_path box1 box2 ... boxN;\n",
        "    Box format: x_min,y_min,x_max,y_max,class_id (no space).\n",
        "    For VOC dataset, try python voc_annotation.py\n",
        "    Here is an example:\n",
        "\n",
        "    path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3\n",
        "    path/to/img2.jpg 120,300,250,600,2\n",
        "    ...\n",
        "\n",
        "    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5\n",
        "    The file model_data/yolo_weights.h5 is used to load pretrained weights.\n",
        "\n",
        "    Modify train.py and start training.\n",
        "    python train.py\n",
        "    Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.\n",
        "\n",
        "If you want to use original pretrained weights for YOLOv3:\n",
        "1. wget https://pjreddie.com/media/files/darknet53.conv.74\n",
        "2. rename it as darknet53.weights\n",
        "3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5\n",
        "4. use model_data/darknet53_weights.h5 in train.py\n",
        "Some issues to know\n",
        "\n",
        "    The test environment is\n",
        "        Python 3.5.2\n",
        "        Keras 2.1.5\n",
        "        tensorflow 1.6.0\n",
        "\n",
        "    Default anchors are used. If you use your own anchors, probably some changes are needed.\n",
        "\n",
        "    The inference result is not totally the same as Darknet but the difference is small.\n",
        "\n",
        "    The speed is slower than Darknet. Replacing PIL with opencv may help a little.\n",
        "\n",
        "    Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.\n",
        "\n",
        "    The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.\n",
        "\n",
        "    For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.\n"
      ]
    }
  ]
}